{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as stream:\n",
    "        try:\n",
    "            config = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return config\n",
    "\n",
    "cfg = load_config('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available')\n",
    "    cfg['device'] = True\n",
    "\n",
    "device = torch.device('cuda' if cfg['device'] else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8b21f77470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = cfg['train_path']\n",
    "dataset =cfg['dataset']\n",
    "n_heldout_users = cfg['heldout_users']\n",
    "\n",
    "seed = cfg['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "\n",
    "    return count\n",
    "\n",
    "#훈련된 모델을 이용해 검증할 데이터를 분리하는 함수입니다.\n",
    "#100개의 액션이 있다면, 그중에 test_prop 비율 만큼을 비워두고, 그것을 모델이 예측할 수 있는지를\n",
    "#확인하기 위함입니다.\n",
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    for _, group in data_grouped_by_user:\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te\n",
    "\n",
    "def numerize(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Preprocess Movielens dataset\n",
      "원본 데이터\n",
      "            user   item        time\n",
      "0            11   4643  1230782529\n",
      "1            11    170  1230782534\n",
      "2            11    531  1230782539\n",
      "3            11    616  1230782542\n",
      "4            11   2140  1230782563\n",
      "...         ...    ...         ...\n",
      "5154466  138493  44022  1260209449\n",
      "5154467  138493   4958  1260209482\n",
      "5154468  138493  68319  1260209720\n",
      "5154469  138493  40819  1260209726\n",
      "5154470  138493  27311  1260209807\n",
      "\n",
      "[5154471 rows x 3 columns]\n",
      "유저별 리뷰수\n",
      "          user  size\n",
      "0          11   376\n",
      "1          14   180\n",
      "2          18    77\n",
      "3          25    91\n",
      "4          31   154\n",
      "...       ...   ...\n",
      "31355  138473    63\n",
      "31356  138475   124\n",
      "31357  138486   137\n",
      "31358  138492    68\n",
      "31359  138493   314\n",
      "\n",
      "[31360 rows x 2 columns]\n",
      "아이템별 리뷰수\n",
      "         item   size\n",
      "0          1  12217\n",
      "1          2   3364\n",
      "2          3    734\n",
      "3          4     43\n",
      "4          5    590\n",
      "...      ...    ...\n",
      "6802  118700     54\n",
      "6803  118900     60\n",
      "6804  118997     52\n",
      "6805  119141    122\n",
      "6806  119145     78\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Load and Preprocess Movielens dataset\")\n",
    "# Load Data\n",
    "DATA_DIR = cfg['train_path']\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
    "print(\"원본 데이터\\n\", raw_data)\n",
    "\n",
    "user_activity = get_count(raw_data, 'user')\n",
    "item_popularity = get_count(raw_data, 'item')\n",
    "\n",
    "print(\"유저별 리뷰수\\n\",user_activity)\n",
    "print(\"아이템별 리뷰수\\n\",item_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BEFORE) unique_uid: [    11     14     18 ... 138486 138492 138493]\n",
      "(AFTER) unique_uid: [ 81259  11986  67552 ...   3671  69383 103755]\n",
      "훈련 데이터에 사용될 사용자 수: 25360\n",
      "검증 데이터에 사용될 사용자 수: 3000\n",
      "테스트 데이터에 사용될 사용자 수: 3000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity['user'].unique()\n",
    "unique_sid = item_popularity['item'].unique()\n",
    "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print(\"(AFTER) unique_uid:\",unique_uid)\n",
    "\n",
    "n_users = unique_uid.size #31360\n",
    "n_heldout_users = cfg['heldout_users']\n",
    "\n",
    "\n",
    "# Split Train/Validation/Test User Indices\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "#주의: 데이터의 수가 아닌 사용자의 수입니다!\n",
    "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
    "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
    "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "##훈련 데이터에 해당하는 아이템들\n",
    "#Train에는 전체 데이터를 사용합니다.\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "##아이템 ID\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_uid.txt'), 'w') as f:\n",
    "    for uid in unique_uid:\n",
    "        f.write('%s\\n' % uid)\n",
    "\n",
    "#Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "\n",
    "\n",
    "\n",
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           uid   sid\n",
      "0        13266     0\n",
      "1        13266     1\n",
      "2        13266     2\n",
      "3        13266     3\n",
      "4        13266     4\n",
      "...        ...   ...\n",
      "5154466   4927   423\n",
      "5154467   4927  1453\n",
      "5154468   4927   331\n",
      "5154469   4927   733\n",
      "5154470   4927  2160\n",
      "\n",
      "[4164260 rows x 2 columns]\n",
      "           uid   sid\n",
      "2132     28125   484\n",
      "2134     28125  1162\n",
      "2135     28125  1418\n",
      "2136     28125   515\n",
      "2137     28125   320\n",
      "...        ...   ...\n",
      "5154152  28265  1890\n",
      "5154153  28265   203\n",
      "5154154  28265  1667\n",
      "5154155  28265   617\n",
      "5154156  28265  2076\n",
      "\n",
      "[394599 rows x 2 columns]\n",
      "           uid   sid\n",
      "2133     28125   887\n",
      "2140     28125  1469\n",
      "2142     28125   265\n",
      "2151     28125   238\n",
      "2152     28125   289\n",
      "...        ...   ...\n",
      "5154117  28265    85\n",
      "5154121  28265   301\n",
      "5154130  28265  1936\n",
      "5154132  28265  2620\n",
      "5154147  28265  1284\n",
      "\n",
      "[97148 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#데이터 셋 확인\n",
    "print(train_data)\n",
    "print(vad_data_tr)\n",
    "print(vad_data_te)\n",
    "# print(test_data_tr)\n",
    "# print(test_data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader():\n",
    "    '''\n",
    "    Load Movielens dataset\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
    "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
    "\n",
    "        self.n_items = self.load_n_items()\n",
    "\n",
    "    def load_data(self, datatype='train'):\n",
    "        if datatype == 'train':\n",
    "            return self._load_train_data()\n",
    "        elif datatype == 'validation':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'test':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        else:\n",
    "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
    "\n",
    "    def load_n_items(self):\n",
    "        unique_sid = list()\n",
    "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                unique_sid.append(line.strip())\n",
    "        n_items = len(unique_sid)\n",
    "        return n_items\n",
    "\n",
    "    def _load_train_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'train.csv')\n",
    "\n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        return data\n",
    "\n",
    "    def _load_tr_te_data(self, datatype='test'):\n",
    "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
    "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
    "\n",
    "        tp_tr = pd.read_csv(tr_path)\n",
    "        tp_te = pd.read_csv(te_path)\n",
    "\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#이미 완성된 MultiDAE의 코드를 참고하여 그 아래 MultiVAE의 코드를 완성해보세요!\n",
    "class MultiDAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-DAE.\n",
    "\n",
    "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
    "        super(MultiDAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims:\n",
    "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        else:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "\n",
    "\n",
    "class MultiVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-VAE.\n",
    "\n",
    "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims:\n",
    "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        else:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "\n",
    "        # Last dimension of q- network is for mean and variance\n",
    "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
    "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
    "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        mu, logvar = self.encode(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def encode(self, input):\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        for i, layer in enumerate(self.q_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.q_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "            else:\n",
    "                mu = h[:, :self.q_dims[-1]]\n",
    "                logvar = h[:, self.q_dims[-1]:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = z\n",
    "        for i, layer in enumerate(self.p_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.p_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.q_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "        for layer in self.p_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "    return BCE + anneal * KLD\n",
    "\n",
    "def loss_function_dae(recon_x, x):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bottleneck as bn\n",
    "import numpy as np\n",
    "\n",
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t\n",
    "\n",
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, is_VAE = False):\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "\n",
    "    for batch_idx, start_idx in enumerate(range(0, N, cfg['batch_size'])):\n",
    "        end_idx = min(start_idx + cfg['batch_size'], N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if is_VAE:\n",
    "          if cfg['total_anneal_steps'] > 0:\n",
    "            anneal = min(cfg['anneal_cap'],\n",
    "                            1. * update_count / cfg['total_anneal_steps'])\n",
    "          else:\n",
    "              anneal = cfg['anneal_cap']\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          recon_batch, mu, logvar = model(data)\n",
    "\n",
    "          loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        else:\n",
    "          recon_batch = model(data)\n",
    "          loss = criterion(recon_batch, data)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % cfg['log_interval'] == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, cfg['batch_size'])),\n",
    "                        elapsed * 1000 / cfg['log_interval'],\n",
    "                        train_loss / cfg['log_interval']))\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    total_val_loss_list = []\n",
    "    n100_list = []\n",
    "    r10_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, cfg['batch_size']):\n",
    "            end_idx = min(start_idx + cfg['batch_size'], N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "            if is_VAE :\n",
    "\n",
    "              if cfg['total_anneal_steps'] > 0:\n",
    "                  anneal = min(cfg['anneal_cap'],\n",
    "                                1. * update_count / cfg['total_anneal_steps'])\n",
    "              else:\n",
    "                  anneal = cfg['anneal_cap']\n",
    "\n",
    "              recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "              loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "\n",
    "            else :\n",
    "              recon_batch = model(data_tensor)\n",
    "              loss = criterion(recon_batch, data_tensor)\n",
    "\n",
    "            total_val_loss_list.append(loss.item())\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r10 = Recall_at_k_batch(recon_batch, heldout_data, 10)\n",
    "            r20 = Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            n100_list.append(n100)\n",
    "            r10_list.append(r10)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    "\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r10_list = np.concatenate(r10_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return np.nanmean(total_val_loss_list), np.nanmean(n100_list), np.nanmean(r10_list), np.nanmean(r20_list), np.nanmean(r50_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "loader = DataLoader(cfg['train_path'])\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = MultiVAE(p_dims).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=cfg['wd'])\n",
    "criterion = loss_function_vae\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "best_n100 = -np.inf\n",
    "update_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2.88s | valid loss 1010.78 | n100 0.274 | r10 0.218 | r20 0.202 | r50 0.249\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 2.06s | valid loss 974.64 | n100 0.324 | r10 0.263 | r20 0.244 | r50 0.297\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 2.04s | valid loss 958.03 | n100 0.349 | r10 0.283 | r20 0.264 | r50 0.319\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 2.07s | valid loss 948.22 | n100 0.372 | r10 0.310 | r20 0.283 | r50 0.340\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 1.98s | valid loss 939.90 | n100 0.384 | r10 0.321 | r20 0.295 | r50 0.354\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 2.09s | valid loss 934.08 | n100 0.396 | r10 0.329 | r20 0.302 | r50 0.363\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 2.02s | valid loss 931.71 | n100 0.398 | r10 0.327 | r20 0.304 | r50 0.367\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 2.12s | valid loss 929.75 | n100 0.401 | r10 0.331 | r20 0.308 | r50 0.370\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 2.13s | valid loss 927.13 | n100 0.406 | r10 0.337 | r20 0.313 | r50 0.374\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 2.13s | valid loss 924.49 | n100 0.411 | r10 0.342 | r20 0.316 | r50 0.380\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 2.13s | valid loss 922.29 | n100 0.414 | r10 0.343 | r20 0.320 | r50 0.385\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 2.14s | valid loss 920.34 | n100 0.420 | r10 0.350 | r20 0.324 | r50 0.389\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 2.14s | valid loss 918.47 | n100 0.423 | r10 0.355 | r20 0.325 | r50 0.391\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 2.02s | valid loss 916.85 | n100 0.424 | r10 0.354 | r20 0.328 | r50 0.394\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 2.10s | valid loss 915.49 | n100 0.426 | r10 0.357 | r20 0.331 | r50 0.395\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 2.06s | valid loss 914.25 | n100 0.427 | r10 0.356 | r20 0.332 | r50 0.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 2.06s | valid loss 913.25 | n100 0.430 | r10 0.360 | r20 0.333 | r50 0.398\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 2.11s | valid loss 912.42 | n100 0.430 | r10 0.357 | r20 0.333 | r50 0.398\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 2.09s | valid loss 911.38 | n100 0.432 | r10 0.361 | r20 0.336 | r50 0.400\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 2.06s | valid loss 910.41 | n100 0.433 | r10 0.360 | r20 0.336 | r50 0.401\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 2.07s | valid loss 909.60 | n100 0.435 | r10 0.360 | r20 0.336 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 2.09s | valid loss 908.88 | n100 0.435 | r10 0.360 | r20 0.335 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 2.15s | valid loss 908.32 | n100 0.437 | r10 0.365 | r20 0.339 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 2.24s | valid loss 907.57 | n100 0.436 | r10 0.363 | r20 0.336 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 2.07s | valid loss 906.91 | n100 0.437 | r10 0.363 | r20 0.338 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 2.11s | valid loss 906.21 | n100 0.439 | r10 0.366 | r20 0.339 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 2.03s | valid loss 905.64 | n100 0.440 | r10 0.364 | r20 0.341 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 2.00s | valid loss 905.09 | n100 0.438 | r10 0.362 | r20 0.339 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 2.10s | valid loss 904.79 | n100 0.439 | r10 0.364 | r20 0.340 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 2.10s | valid loss 904.18 | n100 0.438 | r10 0.358 | r20 0.338 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 2.13s | valid loss 903.60 | n100 0.440 | r10 0.364 | r20 0.341 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 2.10s | valid loss 903.13 | n100 0.438 | r10 0.362 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 2.13s | valid loss 902.80 | n100 0.439 | r10 0.363 | r20 0.341 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 2.13s | valid loss 902.31 | n100 0.440 | r10 0.363 | r20 0.343 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 2.14s | valid loss 901.94 | n100 0.440 | r10 0.362 | r20 0.342 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 2.13s | valid loss 901.43 | n100 0.438 | r10 0.359 | r20 0.342 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 2.16s | valid loss 901.06 | n100 0.442 | r10 0.363 | r20 0.342 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 2.06s | valid loss 900.65 | n100 0.440 | r10 0.360 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 2.11s | valid loss 900.05 | n100 0.439 | r10 0.359 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 1.99s | valid loss 899.79 | n100 0.439 | r10 0.364 | r20 0.340 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 2.12s | valid loss 899.43 | n100 0.440 | r10 0.361 | r20 0.340 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 2.16s | valid loss 899.12 | n100 0.440 | r10 0.359 | r20 0.340 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 2.11s | valid loss 898.75 | n100 0.440 | r10 0.363 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 2.16s | valid loss 898.19 | n100 0.439 | r10 0.357 | r20 0.339 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 2.15s | valid loss 897.81 | n100 0.439 | r10 0.360 | r20 0.339 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 2.03s | valid loss 897.50 | n100 0.440 | r10 0.363 | r20 0.341 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 2.08s | valid loss 897.21 | n100 0.438 | r10 0.358 | r20 0.338 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 2.07s | valid loss 896.85 | n100 0.440 | r10 0.359 | r20 0.340 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 2.18s | valid loss 896.70 | n100 0.440 | r10 0.361 | r20 0.341 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 2.15s | valid loss 896.33 | n100 0.438 | r10 0.356 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 2.02s | valid loss 895.99 | n100 0.438 | r10 0.358 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 2.05s | valid loss 895.67 | n100 0.441 | r10 0.363 | r20 0.340 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 2.09s | valid loss 895.36 | n100 0.440 | r10 0.357 | r20 0.340 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 2.19s | valid loss 894.98 | n100 0.443 | r10 0.364 | r20 0.341 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 2.13s | valid loss 894.92 | n100 0.440 | r10 0.360 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 2.17s | valid loss 894.58 | n100 0.439 | r10 0.360 | r20 0.340 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 2.14s | valid loss 894.05 | n100 0.439 | r10 0.355 | r20 0.339 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 2.10s | valid loss 893.83 | n100 0.441 | r10 0.360 | r20 0.340 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 2.10s | valid loss 893.63 | n100 0.438 | r10 0.355 | r20 0.336 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 2.08s | valid loss 893.26 | n100 0.441 | r10 0.361 | r20 0.340 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 2.09s | valid loss 893.32 | n100 0.441 | r10 0.359 | r20 0.339 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 2.08s | valid loss 893.04 | n100 0.440 | r10 0.358 | r20 0.338 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 2.08s | valid loss 892.81 | n100 0.439 | r10 0.356 | r20 0.338 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 2.12s | valid loss 892.65 | n100 0.438 | r10 0.358 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 2.19s | valid loss 892.40 | n100 0.442 | r10 0.362 | r20 0.338 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 2.04s | valid loss 892.19 | n100 0.439 | r10 0.355 | r20 0.337 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 2.08s | valid loss 891.84 | n100 0.438 | r10 0.353 | r20 0.337 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 2.17s | valid loss 891.63 | n100 0.441 | r10 0.358 | r20 0.338 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 2.15s | valid loss 891.80 | n100 0.439 | r10 0.354 | r20 0.335 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 2.11s | valid loss 891.38 | n100 0.440 | r10 0.359 | r20 0.339 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 2.13s | valid loss 891.24 | n100 0.439 | r10 0.357 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 2.10s | valid loss 891.10 | n100 0.439 | r10 0.358 | r20 0.336 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 2.08s | valid loss 890.89 | n100 0.440 | r10 0.356 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 1.97s | valid loss 891.04 | n100 0.439 | r10 0.357 | r20 0.338 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 1.91s | valid loss 890.42 | n100 0.440 | r10 0.355 | r20 0.336 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 2.23s | valid loss 890.43 | n100 0.439 | r10 0.357 | r20 0.336 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 2.23s | valid loss 890.39 | n100 0.441 | r10 0.360 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 2.07s | valid loss 890.20 | n100 0.439 | r10 0.355 | r20 0.337 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 2.07s | valid loss 890.09 | n100 0.439 | r10 0.357 | r20 0.336 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 2.08s | valid loss 890.03 | n100 0.438 | r10 0.358 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 2.02s | valid loss 890.18 | n100 0.438 | r10 0.355 | r20 0.335 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 2.14s | valid loss 889.71 | n100 0.439 | r10 0.358 | r20 0.337 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 2.10s | valid loss 889.61 | n100 0.439 | r10 0.358 | r20 0.336 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 2.09s | valid loss 889.45 | n100 0.439 | r10 0.358 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 2.19s | valid loss 889.28 | n100 0.440 | r10 0.359 | r20 0.340 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 2.14s | valid loss 889.55 | n100 0.437 | r10 0.353 | r20 0.336 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 2.15s | valid loss 889.36 | n100 0.440 | r10 0.360 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 2.09s | valid loss 889.37 | n100 0.439 | r10 0.357 | r20 0.336 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 2.08s | valid loss 889.11 | n100 0.439 | r10 0.355 | r20 0.336 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 2.18s | valid loss 889.03 | n100 0.440 | r10 0.360 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 2.15s | valid loss 888.62 | n100 0.440 | r10 0.357 | r20 0.338 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 2.10s | valid loss 888.94 | n100 0.441 | r10 0.362 | r20 0.340 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 2.12s | valid loss 888.89 | n100 0.438 | r10 0.356 | r20 0.337 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 2.13s | valid loss 888.82 | n100 0.439 | r10 0.355 | r20 0.336 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 2.12s | valid loss 888.57 | n100 0.440 | r10 0.357 | r20 0.337 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 2.14s | valid loss 888.60 | n100 0.439 | r10 0.358 | r20 0.337 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 2.14s | valid loss 888.50 | n100 0.440 | r10 0.359 | r20 0.336 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 2.00s | valid loss 888.58 | n100 0.439 | r10 0.356 | r20 0.337 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 2.07s | valid loss 888.47 | n100 0.441 | r10 0.357 | r20 0.337 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 2.02s | valid loss 888.34 | n100 0.439 | r10 0.358 | r20 0.338 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss 914.11 | n100 0.44 | r10 0.36 | r20 0.34 | r50 0.41\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, cfg['epochs'] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, criterion, optimizer, is_VAE=True)\n",
    "    val_loss, n100, r10, r20, r50 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=True)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                epoch, time.time() - epoch_start_time, val_loss,\n",
    "                n100, r10, r20, r50))\n",
    "    print('-' * 89)\n",
    "\n",
    "    n_iter = epoch * len(range(0, N, cfg['batch_size']))\n",
    "\n",
    "\n",
    "    # Save the model if the n100 is the best we've seen so far.\n",
    "    if n100 > best_n100:\n",
    "        with open(cfg['save'], 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_n100 = n100\n",
    "\n",
    "\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(cfg['save'], 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss, n100, r10, r20, r50 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=True)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
    "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv is saved!\n"
     ]
    }
   ],
   "source": [
    "with open(cfg['save'], 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# save submittion top 10 per user\n",
    "sub_data = train_data\n",
    "sub_data_tensor = naive_sparse2tensor(sub_data).to(device)\n",
    "recon_batch, _, _ = model(sub_data_tensor)\n",
    "recon_batch = recon_batch.cpu().detach().numpy()\n",
    "\n",
    "sub_data_cpu = sub_data_tensor.cpu()\n",
    "\n",
    "# recon_batch[sub_data.nonzero()] = -np.inf\n",
    "\n",
    "topk = np.argsort(-recon_batch, axis=1)[:, :10]\n",
    "sub = pd.DataFrame(topk)\n",
    "sub.to_csv('submission.csv', index=False, header=False)\n",
    "print(\"submission.csv is saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "\n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "\n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "\n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "\n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "\n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "\n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "\n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)\n",
    "\n",
    "\n",
    "with open(cfg['save'], 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model.eval()\n",
    "total_batch=[]\n",
    "first=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑 코드 data_in : train_data, vad_data_tr, test_data_tr 한번씩 바꿔가면서 3번돌림 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31360, 6807)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in generate(batch_size=500,\n",
    "                              device=device,\n",
    "                              data_in=test_data_tr,\n",
    "                              #data_out=test_data_te,\n",
    "                              samples_perc_per_epoch=1\n",
    "                             ):\n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_pred, _, _ = model(ratings_in)#.cpu().detach().numpy()\n",
    "        ratings_pred = ratings_pred.cpu().detach().numpy()\n",
    "        if first:\n",
    "            total_batch=ratings_pred\n",
    "            first = False\n",
    "        else:\n",
    "            total_batch=np.concatenate([total_batch,ratings_pred],axis=0)\n",
    "\n",
    "print(total_batch.shape)\n",
    "\n",
    "temp=pd.DataFrame(total_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_sid=pd.read_csv(os.path.join(cfg['train_path'], 'pro_sg/unique_sid.txt'),sep=\" \",header=None)\n",
    "unique_uid=pd.read_csv(os.path.join(cfg['train_path'], 'pro_sg/unique_uid.txt'),sep=\" \",header=None)\n",
    "\n",
    "# unique_uid=pd.read_csv('/opt/ml/input/RECVAE/data/train/Preprocessed/unique_uid.txt',sep=\" \",header=None)\n",
    "\n",
    "id2show = dict((i, sid) for (i, sid) in enumerate(unique_sid.squeeze()))\n",
    "id2profile = dict((i, pid) for (i, pid) in enumerate(unique_uid.squeeze()))\n",
    "\n",
    "column=list(temp.columns)\n",
    "origin_mid=[id2show[x] for x in column]\n",
    "\n",
    "row=list(temp.index)\n",
    "origin_uid=[id2profile[x] for x in row]\n",
    "\n",
    "temp.columns=origin_mid\n",
    "temp.index=origin_uid\n",
    "# raw_data=pd.read_csv('/opt/ml/input/RECVAE/data/train/train_ratings.csv')\n",
    "\n",
    "\n",
    "watchedm=raw_data.groupby('user')['item'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31360/31360 [01:05<00:00, 478.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "sumbission=dict()\n",
    "sumbission={'user': [],'item': []}\n",
    "sumbission\n",
    "\n",
    "for row in tqdm(temp.iterrows(),total=31360):\n",
    "    userid=row[0]\n",
    "    movies=row[1]\n",
    "    watchedmovies=watchedm.get(userid, [])\n",
    "\n",
    "    for _ in range(10):\n",
    "        sumbission['user'].append(userid)\n",
    "\n",
    "    itemp=[]\n",
    "    for movie in reversed(list(movies.sort_values().index)):\n",
    "        if len(itemp)==10:\n",
    "            break\n",
    "        else:\n",
    "            if movie not in watchedmovies:\n",
    "                itemp.append(movie)\n",
    "\n",
    "    sumbission['item']+=itemp\n",
    "\n",
    "sumbission=pd.DataFrame(sumbission)\n",
    "sumbission.sort_values('user', inplace=True)\n",
    "sumbission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
