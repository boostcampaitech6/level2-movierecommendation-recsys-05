{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import bottleneck as bn\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as stream:\n",
    "        try:\n",
    "            config = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return config\n",
    "\n",
    "cfg = load_config('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available')\n",
    "    cfg['device'] = True\n",
    "\n",
    "device = torch.device('cuda' if cfg['device'] else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd5e8996490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = cfg['train_path']\n",
    "dataset =cfg['dataset']\n",
    "n_heldout_users = cfg['heldout_users']\n",
    "\n",
    "seed = cfg['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "\n",
    "    return count\n",
    "\n",
    "#훈련된 모델을 이용해 검증할 데이터를 분리하는 함수입니다.\n",
    "#100개의 액션이 있다면, 그중에 test_prop 비율 만큼을 비워두고, 그것을 모델이 예측할 수 있는지를\n",
    "#확인하기 위함입니다.\n",
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    for _, group in data_grouped_by_user:\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te\n",
    "\n",
    "def numerize(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Preprocess Movielens dataset\n",
      "원본 데이터\n",
      "            user   item        time\n",
      "0            11   4643  1230782529\n",
      "1            11    170  1230782534\n",
      "2            11    531  1230782539\n",
      "3            11    616  1230782542\n",
      "4            11   2140  1230782563\n",
      "...         ...    ...         ...\n",
      "5154466  138493  44022  1260209449\n",
      "5154467  138493   4958  1260209482\n",
      "5154468  138493  68319  1260209720\n",
      "5154469  138493  40819  1260209726\n",
      "5154470  138493  27311  1260209807\n",
      "\n",
      "[5154471 rows x 3 columns]\n",
      "유저별 리뷰수\n",
      "          user  size\n",
      "0          11   376\n",
      "1          14   180\n",
      "2          18    77\n",
      "3          25    91\n",
      "4          31   154\n",
      "...       ...   ...\n",
      "31355  138473    63\n",
      "31356  138475   124\n",
      "31357  138486   137\n",
      "31358  138492    68\n",
      "31359  138493   314\n",
      "\n",
      "[31360 rows x 2 columns]\n",
      "아이템별 리뷰수\n",
      "         item   size\n",
      "0          1  12217\n",
      "1          2   3364\n",
      "2          3    734\n",
      "3          4     43\n",
      "4          5    590\n",
      "...      ...    ...\n",
      "6802  118700     54\n",
      "6803  118900     60\n",
      "6804  118997     52\n",
      "6805  119141    122\n",
      "6806  119145     78\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Load and Preprocess Movielens dataset\")\n",
    "# Load Data\n",
    "DATA_DIR = cfg['train_path']\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
    "print(\"원본 데이터\\n\", raw_data)\n",
    "\n",
    "user_activity = get_count(raw_data, 'user')\n",
    "item_popularity = get_count(raw_data, 'item')\n",
    "\n",
    "print(\"유저별 리뷰수\\n\",user_activity)\n",
    "print(\"아이템별 리뷰수\\n\",item_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BEFORE) unique_uid: [    11     14     18 ... 138486 138492 138493]\n",
      "(AFTER) unique_uid: [ 81259  11986  67552 ...   3671  69383 103755]\n",
      "훈련 데이터에 사용될 사용자 수: 25360\n",
      "검증 데이터에 사용될 사용자 수: 3000\n",
      "테스트 데이터에 사용될 사용자 수: 3000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity['user'].unique()\n",
    "unique_sid = item_popularity['item'].unique()\n",
    "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print(\"(AFTER) unique_uid:\",unique_uid)\n",
    "\n",
    "n_users = unique_uid.size #31360\n",
    "n_heldout_users = cfg['heldout_users']\n",
    "\n",
    "\n",
    "# Split Train/Validation/Test User Indices\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "#주의: 데이터의 수가 아닌 사용자의 수입니다!\n",
    "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
    "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
    "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "##훈련 데이터에 해당하는 아이템들\n",
    "#Train에는 전체 데이터를 사용합니다.\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "##아이템 ID\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_uid.txt'), 'w') as f:\n",
    "    for uid in unique_uid:\n",
    "        f.write('%s\\n' % uid)\n",
    "\n",
    "#Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "\n",
    "\n",
    "\n",
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    Load Movielens dataset\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
    "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
    "\n",
    "        self.n_items = self.load_n_items()\n",
    "\n",
    "    def load_data(self, datatype='train'):\n",
    "        if datatype == 'train':\n",
    "            return self._load_train_data()\n",
    "        elif datatype == 'validation':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'test':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        else:\n",
    "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
    "\n",
    "    def load_n_items(self):\n",
    "        unique_sid = list()\n",
    "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                unique_sid.append(line.strip())\n",
    "        n_items = len(unique_sid)\n",
    "        return n_items\n",
    "\n",
    "    def _load_train_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'train.csv')\n",
    "\n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        return data\n",
    "\n",
    "    def _load_tr_te_data(self, datatype='test'):\n",
    "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
    "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
    "\n",
    "        tp_tr = pd.read_csv(tr_path)\n",
    "        tp_te = pd.read_csv(te_path)\n",
    "\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ndcg(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def recall(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "        super(CompositePrior, self).__init__()\n",
    "\n",
    "        self.mixture_weights = mixture_weights\n",
    "\n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.mu_prior.data.fill_(0)\n",
    "\n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "\n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "\n",
    "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.encoder_old.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "\n",
    "        post_mu, post_logvar = self.encoder_old(x, dropout_rate = 0)\n",
    "\n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "\n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "\n",
    "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "\n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "\n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "\n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
    "\n",
    "\n",
    "class RecVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 600, latent_dim = 200):\n",
    "        super(RecVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, user_ratings, beta=None, gamma=0.005, dropout_rate=0.5, calculate_loss=True):\n",
    "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred = self.decoder(z)\n",
    "\n",
    "        if calculate_loss:\n",
    "            if gamma:\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                kl_weight = gamma * norm\n",
    "            elif beta:\n",
    "                kl_weight = beta\n",
    "\n",
    "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "            negative_elbo = -(mll - kld)\n",
    "\n",
    "            return (mll, kld), negative_elbo\n",
    "\n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "\n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "\n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "\n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "\n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "\n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "\n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "\n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)\n",
    "\n",
    "\n",
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "    metrics = deepcopy(metrics)\n",
    "    model.eval()\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = []\n",
    "\n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "\n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "\n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "\n",
    "        if not (data_in is data_out):\n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "\n",
    "        for m in metrics:\n",
    "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = np.concatenate(m['score']).mean()\n",
    "\n",
    "    return [x['score'] for x in metrics]\n",
    "\n",
    "\n",
    "def train(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
    "            loss.backward()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(cfg['train_path'])\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "model = RecVAE(input_dim=n_items, hidden_dim=cfg['hidden_dim'], latent_dim=cfg['latent_dim']).to(device)\n",
    "model_best = RecVAE(input_dim=n_items, hidden_dim=cfg['hidden_dim'], latent_dim=cfg['latent_dim']).to(device)\n",
    "\n",
    "decoder_params = set(model.decoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())\n",
    "\n",
    "lr = np.float32(cfg['lr'])\n",
    "\n",
    "optimizer_encoder = optim.Adam(encoder_params, lr=lr)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=lr)\n",
    "opts = [optim.Adam(model.parameters(), lr=lr), optimizer_encoder, optimizer_decoder]\n",
    "\n",
    "metrics = [{'metric': ndcg, 'k': 10}, {'metric': recall, 'k': 10}]\n",
    "\n",
    "best_r10 = -np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | valid ndcg@10: 0.3261 |  valid recall@10: 0.2978 | best valid recall@10: 0.2978 | train ndcg@10: 0.7080 | train recall@10: 0.6901 \n",
      "epoch 1 | valid ndcg@10: 0.3635 |  valid recall@10: 0.3334 | best valid recall@10: 0.3334 | train ndcg@10: 0.7553 | train recall@10: 0.7399 \n",
      "epoch 2 | valid ndcg@10: 0.3752 |  valid recall@10: 0.3430 | best valid recall@10: 0.3430 | train ndcg@10: 0.7671 | train recall@10: 0.7439 \n",
      "epoch 3 | valid ndcg@10: 0.3845 |  valid recall@10: 0.3521 | best valid recall@10: 0.3521 | train ndcg@10: 0.7749 | train recall@10: 0.7530 \n",
      "epoch 4 | valid ndcg@10: 0.3908 |  valid recall@10: 0.3573 | best valid recall@10: 0.3573 | train ndcg@10: 0.7744 | train recall@10: 0.7545 \n",
      "epoch 5 | valid ndcg@10: 0.3907 |  valid recall@10: 0.3572 | best valid recall@10: 0.3573 | train ndcg@10: 0.7805 | train recall@10: 0.7605 \n",
      "epoch 6 | valid ndcg@10: 0.3964 |  valid recall@10: 0.3633 | best valid recall@10: 0.3633 | train ndcg@10: 0.7848 | train recall@10: 0.7672 \n",
      "epoch 7 | valid ndcg@10: 0.3978 |  valid recall@10: 0.3642 | best valid recall@10: 0.3642 | train ndcg@10: 0.7977 | train recall@10: 0.7787 \n",
      "epoch 8 | valid ndcg@10: 0.4013 |  valid recall@10: 0.3663 | best valid recall@10: 0.3663 | train ndcg@10: 0.7930 | train recall@10: 0.7763 \n",
      "epoch 9 | valid ndcg@10: 0.4018 |  valid recall@10: 0.3682 | best valid recall@10: 0.3682 | train ndcg@10: 0.7896 | train recall@10: 0.7711 \n",
      "epoch 10 | valid ndcg@10: 0.4012 |  valid recall@10: 0.3681 | best valid recall@10: 0.3682 | train ndcg@10: 0.8050 | train recall@10: 0.7877 \n",
      "epoch 11 | valid ndcg@10: 0.4058 |  valid recall@10: 0.3720 | best valid recall@10: 0.3720 | train ndcg@10: 0.8090 | train recall@10: 0.7921 \n",
      "epoch 12 | valid ndcg@10: 0.4069 |  valid recall@10: 0.3728 | best valid recall@10: 0.3728 | train ndcg@10: 0.8076 | train recall@10: 0.7893 \n",
      "epoch 13 | valid ndcg@10: 0.4109 |  valid recall@10: 0.3753 | best valid recall@10: 0.3753 | train ndcg@10: 0.8123 | train recall@10: 0.7980 \n",
      "epoch 14 | valid ndcg@10: 0.4086 |  valid recall@10: 0.3753 | best valid recall@10: 0.3753 | train ndcg@10: 0.8181 | train recall@10: 0.7972 \n",
      "epoch 15 | valid ndcg@10: 0.4110 |  valid recall@10: 0.3751 | best valid recall@10: 0.3753 | train ndcg@10: 0.8112 | train recall@10: 0.7929 \n",
      "epoch 16 | valid ndcg@10: 0.4144 |  valid recall@10: 0.3797 | best valid recall@10: 0.3797 | train ndcg@10: 0.8121 | train recall@10: 0.7929 \n",
      "epoch 17 | valid ndcg@10: 0.4122 |  valid recall@10: 0.3788 | best valid recall@10: 0.3797 | train ndcg@10: 0.8161 | train recall@10: 0.8004 \n",
      "epoch 18 | valid ndcg@10: 0.4127 |  valid recall@10: 0.3788 | best valid recall@10: 0.3797 | train ndcg@10: 0.8121 | train recall@10: 0.7960 \n",
      "epoch 19 | valid ndcg@10: 0.4130 |  valid recall@10: 0.3787 | best valid recall@10: 0.3797 | train ndcg@10: 0.8147 | train recall@10: 0.7992 \n",
      "ndcg@10:\t0.4132\n",
      "recall@10:\t0.3776\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 'input_dim': train_data.shape[1]\n",
    "\n",
    "# train_scores, valid_scores = [], []\n",
    "\n",
    "learning_kwargs = {\n",
    "    'model': model,\n",
    "    'train_data': train_data,\n",
    "    'batch_size': cfg['batch_size'],\n",
    "    'beta': cfg['beta'],\n",
    "    'gamma': cfg['gamma']\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(cfg['epochs']):\n",
    "    train_scores, valid_scores = [], []\n",
    "    if cfg['not_alternating']:\n",
    "        train(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=cfg['dropout'], **learning_kwargs)\n",
    "    else:\n",
    "        train(opts=[optimizer_encoder], n_epochs=cfg['e_num_epochs'], dropout_rate=cfg['dropout'] **learning_kwargs)\n",
    "        model.update_prior()\n",
    "        train(opts=[optimizer_decoder], n_epochs=cfg['d_num_epochs'], dropout_rate=0, **learning_kwargs)\n",
    "\n",
    "    train_scores=evaluate(model, train_data, train_data, metrics, 0.01)\n",
    "\n",
    "    valid_scores=evaluate(model, vad_data_tr, vad_data_te, metrics, 1)\n",
    "\n",
    "\n",
    "    if valid_scores[-1] > best_r10:\n",
    "        best_r10 = valid_scores[-1]\n",
    "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "        torch.save(model_best,'model{}.pt'.format(epoch))\n",
    "\n",
    "    print(f'epoch {epoch} | valid ndcg@10: {valid_scores[0]:.4f} |  valid recall@10: {valid_scores[1]:.4f} | ' +\n",
    "          f'best valid recall@10: {best_r10:.4f} | train ndcg@10: {train_scores[0]:.4f} | train recall@10: {train_scores[1]:.4f} ')\n",
    "\n",
    "\n",
    "\n",
    "test_metrics = [{'metric': ndcg, 'k': 10}, {'metric': recall, 'k': 10}]\n",
    "\n",
    "final_scores = evaluate(model_best, test_data_tr, test_data_te, test_metrics)\n",
    "\n",
    "for metric, score in zip(test_metrics, final_scores):\n",
    "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "# ax = ax.flatten()\n",
    "# epochs = [i for i in range(1, config.num_epochs + 1)]\n",
    "\n",
    "# ax[0].plot(epochs, loss_list)\n",
    "# ax[0].set_title('Loss')\n",
    "\n",
    "# ax[1].plot(epochs, ndcg_list)\n",
    "# ax[1].set_title('NDCG')\n",
    "\n",
    "# ax[2].plot(epochs, hit_list)\n",
    "# ax[2].set_title('HIT')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model16.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "model.eval()\n",
    "total_batch=[]\n",
    "first=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것도 세번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31360, 6807)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in generate(batch_size=500,\n",
    "                              device=device,\n",
    "                              data_in=test_data_tr,\n",
    "                              #data_out=test_data_te,\n",
    "                              samples_perc_per_epoch=1\n",
    "                             ):\n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_pred = model(ratings_in, calculate_loss=False)#.cpu().detach().numpy()\n",
    "        ratings_pred = ratings_pred.cpu().detach().numpy()\n",
    "        if first:\n",
    "            total_batch=ratings_pred\n",
    "            first = False\n",
    "        else:\n",
    "            total_batch=np.concatenate([total_batch,ratings_pred],axis=0)\n",
    "\n",
    "print(total_batch.shape)\n",
    "\n",
    "temp=pd.DataFrame(total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid=pd.read_csv(os.path.join(cfg['train_path'], 'pro_sg/unique_sid.txt'),sep=\" \",header=None)\n",
    "unique_uid=pd.read_csv(os.path.join(cfg['train_path'], 'pro_sg/unique_uid.txt'),sep=\" \",header=None)\n",
    "\n",
    "# unique_uid=pd.read_csv('/opt/ml/input/RECVAE/data/train/Preprocessed/unique_uid.txt',sep=\" \",header=None)\n",
    "\n",
    "id2show = dict((i, sid) for (i, sid) in enumerate(unique_sid.squeeze()))\n",
    "id2profile = dict((i, pid) for (i, pid) in enumerate(unique_uid.squeeze()))\n",
    "\n",
    "column=list(temp.columns)\n",
    "origin_mid=[id2show[x] for x in column]\n",
    "\n",
    "row=list(temp.index)\n",
    "origin_uid=[id2profile[x] for x in row]\n",
    "\n",
    "temp.columns=origin_mid\n",
    "temp.index=origin_uid\n",
    "# raw_data=pd.read_csv('/opt/ml/input/RECVAE/data/train/train_ratings.csv')\n",
    "\n",
    "\n",
    "watchedm=raw_data.groupby('user')['item'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31360/31360 [01:07<00:00, 463.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "sumbission=dict()\n",
    "sumbission={'user': [],'item': []}\n",
    "sumbission\n",
    "\n",
    "for row in tqdm(temp.iterrows(),total=31360):\n",
    "    userid=row[0]\n",
    "    movies=row[1]\n",
    "    watchedmovies=watchedm.get(userid, [])\n",
    "\n",
    "    for _ in range(10):\n",
    "        sumbission['user'].append(userid)\n",
    "\n",
    "    itemp=[]\n",
    "    for movie in reversed(list(movies.sort_values().index)):\n",
    "        if len(itemp)==10:\n",
    "            break\n",
    "        else:\n",
    "            if movie not in watchedmovies:\n",
    "                itemp.append(movie)\n",
    "\n",
    "    sumbission['item']+=itemp\n",
    "\n",
    "sumbission=pd.DataFrame(sumbission)\n",
    "sumbission.sort_values('user', inplace=True)\n",
    "sumbission.to_csv('submission_recvae.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
